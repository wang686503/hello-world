{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XJCCUgRky5b-"
   },
   "source": [
    "# Comp 551 Tutorial 2 :: scikit-learn\n",
    "\n",
    "Oct 3, 2018\n",
    "\n",
    "### Things we'll cover today\n",
    "\n",
    "1) **What is scikit-learn and why should I care about it**\n",
    "\n",
    "2) **How to go about doing ML (with scikit learn)**\n",
    "  - ML as a single pipeline\n",
    "    - Most common data preprocessing steps\n",
    "      - train-test split\n",
    "      - vectorization of textual features (only for textual features)\n",
    "      - TF-IDF (only for textual features)\n",
    "      - normalization\n",
    "      - one-hot encoding of labels (for classification problems only)\n",
    "    - Training\n",
    "      - fit (closed form or gradient descent)\n",
    "      - predict\n",
    "    - Evaluation\n",
    "      - metrics : measure accuracy - precision / recall / f-score\n",
    "      - Cross validation\n",
    "      - Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HzgjkqOj3OxL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DWojGlHdy5b-"
   },
   "source": [
    "1) **What is scikit-learn and why should I care about it**\n",
    "\n",
    "- Scikit-learn is a python-based free ML library that provides well-implemented off-the-shelf implementations for almost all ML operations.\n",
    "- Implementing ML from scratch that is scalable, efficient, and error-free is really very very hard.\n",
    "\n",
    "*Disclaimer* : Availability of off-the-shelf implementations of ML doesn't invalidates the need to know the algorithms details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "A7QH4_2J3PJy",
    "outputId": "d8bd170e-f067-47f0-cdfc-10536d8d01e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lala\n"
     ]
    }
   ],
   "source": [
    "print(\"lala\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BBbKHE0daYbb"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M8TjcfudaXuM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0J0ZH_ePy5b_"
   },
   "source": [
    "2) **How to go about doing ML (with scikit learn)**\n",
    "\n",
    "> Indented block\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kjxtPBSvy5cB"
   },
   "source": [
    "2.1) **ML as a single pipeline**\n",
    "- Almost all *scalable* ML models follows the style of development in a pipeline. It eases the pain of thinking through the complex ML processes.\n",
    "- Keep this pipeline in mind while developing any ML model.\n",
    "\n",
    "![alt text](http://cs.mcgill.ca/~sthaku3/other_media/COMP551_tutorial_2/Steps.png)\n",
    "\n",
    "\n",
    "- P.S. :: Closed-form solution seeking ML don't follow this suit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f3AiQWOLy5cC"
   },
   "source": [
    "From here on, we'll explain the concepts behind each of these steps with a real dataset called News20group as hosted [here](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html). So, let's import it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "klEFnYucy5cD",
    "outputId": "531495cd-07db-4eaa-9695-47390bad27a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(subset='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fJxE_8DD9oL0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "_icjdV9Sy5cH",
    "outputId": "ef200e85-1907-46a6-ac64-5af88146a441"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From: mblawson@midway.ecn.uoknor.edu (Matthew B Lawson)\\nSubject: Which high-performance VLB video card?\\nSummary: Seek recommendations for VLB video card\\nNntp-Posting-Host: midway.ecn.uoknor.edu\\nOrganization: Engineering Computer Network, University of Oklahoma, Norman, OK, USA\\nKeywords: orchid, stealth, vlb\\nLines: 21\\n\\n  My brother is in the market for a high-performance video card that supports\\nVESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:\\n\\n  - Diamond Stealth Pro Local Bus\\n\\n  - Orchid Farenheit 1280\\n\\n  - ATI Graphics Ultra Pro\\n\\n  - Any other high-performance VLB card\\n\\n\\nPlease post or email.  Thank you!\\n\\n  - Matt\\n\\n-- \\n    |  Matthew B. Lawson <------------> (mblawson@essex.ecn.uoknor.edu)  |   \\n  --+-- \"Now I, Nebuchadnezzar, praise and exalt and glorify the King  --+-- \\n    |   of heaven, because everything he does is right and all his ways  |   \\n    |   are just.\" - Nebuchadnezzar, king of Babylon, 562 B.C.           |   \\n',\n",
       " 'From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject: Re: ARMENIA SAYS IT COULD SHOOT DOWN TURKISH PLANES (Henrik)\\nLines: 95\\nNntp-Posting-Host: viktoria.dsv.su.se\\nReply-To: hilmi-er@dsv.su.se (Hilmi Eren)\\nOrganization: Dept. of Computer and Systems Sciences, Stockholm University\\n\\n\\n\\n\\n|>The student of \"regional killings\" alias Davidian (not the Davidian religios sect) writes:\\n\\n\\n|>Greater Armenia would stretch from Karabakh, to the Black Sea, to the\\n|>Mediterranean, so if you use the term \"Greater Armenia\" use it with care.\\n\\n\\n\\tFinally you said what you dream about. Mediterranean???? That was new....\\n\\tThe area will be \"greater\" after some years, like your \"holocaust\" numbers......\\n\\n\\n\\n\\n|>It has always been up to the Azeris to end their announced winning of Karabakh \\n|>by removing the Armenians! When the president of Azerbaijan, Elchibey, came to \\n|>power last year, he announced he would be be \"swimming in Lake Sevan [in \\n|>Armeniaxn] by July\".\\n\\t\\t*****\\n\\tIs\\'t July in USA now????? Here in Sweden it\\'s April and still cold.\\n\\tOr have you changed your calendar???\\n\\n\\n|>Well, he was wrong! If Elchibey is going to shell the \\n|>Armenians of Karabakh from Aghdam, his people will pay the price! If Elchibey \\n\\t\\t\\t\\t\\t\\t    ****************\\n|>is going to shell Karabakh from Fizuli his people will pay the price! If \\n\\t\\t\\t\\t\\t\\t    ******************\\n|>Elchibey thinks he can get away with bombing Armenia from the hills of \\n|>Kelbajar, his people will pay the price. \\n\\t\\t\\t    ***************\\n\\n\\n\\tNOTHING OF THE MENTIONED IS TRUE, BUT LET SAY IT\\'s TRUE.\\n\\t\\n\\tSHALL THE AZERI WOMEN AND CHILDREN GOING TO PAY THE PRICE WITH\\n\\t\\t\\t\\t\\t\\t    **************\\n\\tBEING RAPED, KILLED AND TORTURED BY THE ARMENIANS??????????\\n\\t\\n\\tHAVE YOU HEARDED SOMETHING CALLED: \"GENEVA CONVENTION\"???????\\n\\tYOU FACIST!!!!!\\n\\n\\n\\n\\tOhhh i forgot, this is how Armenians fight, nobody has forgot\\n\\tyou killings, rapings and torture against the Kurds and Turks once\\n\\tupon a time!\\n      \\n       \\n\\n|>And anyway, this \"60 \\n|>Kurd refugee\" story, as have other stories, are simple fabrications sourced in \\n|>Baku, modified in Ankara. Other examples of this are Armenia has no border \\n|>with Iran, and the ridiculous story of the \"intercepting\" of Armenian military \\n|>conversations as appeared in the New York Times supposedly translated by \\n|>somebody unknown, from Armenian into Azeri Turkish, submitted by an unnamed \\n|>\"special correspondent\" to the NY Times from Baku. Real accurate!\\n\\nOhhhh so swedish RedCross workers do lie they too? What ever you say\\n\"regional killer\", if you don\\'t like the person then shoot him that\\'s your policy.....l\\n\\n\\n|>[HE]\\tSearch Turkish planes? You don\\'t know what you are talking about.<-------\\n|>[HE]\\tsince it\\'s content is announced to be weapons? \\t\\t\\t\\ti\\t \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ti\\n|>Well, big mouth Ozal said military weapons are being provided to Azerbaijan\\ti\\n|>from Turkey, yet Demirel and others say no. No wonder you are so confused!\\ti\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ti\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ti\\n\\tConfused?????\\t\\t\\t\\t\\t\\t\\t\\ti\\n\\tYou facist when you delete text don\\'t change it, i wrote:\\t\\ti\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ti\\n        Search Turkish planes? You don\\'t know what you are talking about.\\ti\\n        Turkey\\'s government has announced that it\\'s giving weapons  <-----------i\\n        to Azerbadjan since Armenia started to attack Azerbadjan\\t\\t\\n        it self, not the Karabag province. So why search a plane for weapons\\t\\n        since it\\'s content is announced to be weapons?   \\n\\n\\tIf there is one that\\'s confused then that\\'s you! We have the right (and we do)\\n\\tto give weapons to the Azeris, since Armenians started the fight in Azerbadjan!\\n \\n\\n|>You are correct, all Turkish planes should be simply shot down! Nice, slow\\n|>moving air transports!\\n\\n\\tShoot down with what? Armenian bread and butter? Or the arms and personel \\n\\tof the Russian army?\\n\\n\\n\\n\\nHilmi Eren\\nStockholm University\\n',\n",
       " 'From: guyd@austin.ibm.com (Guy Dawson)\\nSubject: Re: IDE vs SCSI, DMA and detach\\nOriginator: guyd@pal500.austin.ibm.com\\nOrganization: IBM Austin\\nLines: 60\\n\\n\\nIn article <1993Apr19.034517.12820@julian.uwo.ca>, wlsmith@valve.heart.rri.uwo.ca (Wayne Smith) writes:\\n> In article <RICHK.93Apr15075248@gozer.grebyn.com> richk@grebyn.com (Richard Krehbiel) writes:\\n> >>     Can anyone explain in fairly simple terms why, if I get OS/2, I might \\n> >>   need an SCSI controler rather than an IDE.  Will performance suffer that\\n> >>   much?  For a 200MB or so drive?  If I don\\'t have a tape drive or CD-ROM?\\n> >>   Any help would be appreciated.\\n> \\n> >So, when you\\'ve got multi-tasking, you want to increase performance by\\n> >increasing the amount of overlapping you do.\\n> >\\n> >One way is with DMA or bus mastering.  Either of these make it\\n> >possible for I/O devices to move their data into and out of memory\\n> >without interrupting the CPU.  The alternative is for the CPU to move\\n> >the data.  There are several SCSI interface cards that allow DMA and\\n> >bus mastering.\\n>  ^^^^^^^^^^^^\\n> How do you do bus-mastering on the ISA bus?\\n> \\n> >IDE, however, is defined by the standard AT interface\\n> >created for the IBM PC AT, which requires the CPU to move all the data\\n> >bytes, with no DMA.\\n> \\n> If we\\'re talking ISA (AT) bus here, then you can only have 1 DMA channel\\n> active at any one time, presumably transferring data from a single device.\\n> So even though you can have at least 7 devices on a SCSI bus, explain how\\n> all 7 of those devices can to DMA transfers through a single SCSI card\\n> to the ISA-AT bus at the same time.\\n\\nThink!\\n\\nIt\\'s the SCSI card doing the DMA transfers NOT the disks...\\n\\nThe SCSI card can do DMA transfers containing data from any of the SCSI devices\\nit is attached when it wants to.\\n\\nAn important feature of SCSI is the ability to detach a device. This frees the\\nSCSI bus for other devices. This is typically used in a multi-tasking OS to\\nstart transfers on several devices. While each device is seeking the data the\\nbus is free for other commands and data transfers. When the devices are\\nready to transfer the data they can aquire the bus and send the data.\\n\\nOn an IDE bus when you start a transfer the bus is busy until the disk has seeked\\nthe data and transfered it. This is typically a 10-20ms second lock out for other\\nprocesses wanting the bus irrespective of transfer time.\\n\\n> \\n> Also, I\\'m still trying to track down a copy of IBM\\'s AT reference book,\\n> but from their PC technical manual (page 2-93):\\n> \\n> \"The (FDD) adapter is buffered on the I.O bus and uses the System Board\\n> direct memory access (DMA) for record data transfers.\"\\n> I expect to see something similar for the PC-AT HDD adapter.  \\n> So the lowly low-density original PC FDD card used DMA and the PC-AT\\n> HDD controller doesn\\'t!?!?  That makes real sense.\\n-- \\n-- -----------------------------------------------------------------------------\\nGuy Dawson - Hoskyns Group Plc.\\n        guyd@hoskyns.co.uk  Tel Hoskyns UK     -  71 251 2128\\n        guyd@austin.ibm.com Tel IBM Austin USA - 512 838 3377\\n',\n",
       " 'From: Alexander Samuel McDiarmid <am2o+@andrew.cmu.edu>\\nSubject: driver ??\\nOrganization: Sophomore, Mechanical Engineering, Carnegie Mellon, Pittsburgh, PA\\nLines: 15\\nNNTP-Posting-Host: po4.andrew.cmu.edu\\n\\n \\n1)    I have an old Jasmine drive which I cannot use with my new system.\\n My understanding is that I have to upsate the driver with a more modern\\none in order to gain compatability with system 7.0.1.  does anyone know\\nof an inexpensive program to do this?  ( I have seen formatters for <$20\\nbuit have no idea if they will work)\\n \\n2)     I have another ancient device, this one a tape drive for which\\nthe back utility freezes the system if I try to use it.  THe drive is a\\njasmine direct tape (bought used for $150 w/ 6 tapes, techmar\\nmechanism).  Essentially I have the same question as above, anyone know\\nof an inexpensive beckup utility I can use with system 7.0.1\\n \\nall help and advice appriciated.\\n\\n',\n",
       " 'From: tell@cs.unc.edu (Stephen Tell)\\nSubject: Re: subliminal message flashing on TV\\nOrganization: The University of North Carolina at Chapel Hill\\nLines: 25\\nNNTP-Posting-Host: rukbat.cs.unc.edu\\n\\nIn article <7480237@hpfcso.FC.HP.COM> myers@hpfcso.FC.HP.COM (Bob Myers) writes:\\n>> Hi.  I was doing research on subliminal suggestion for a psychology\\n>> paper, and I read that one researcher flashed hidden messages on the\\n>> TV screen at 1/200ths of a second.  Is that possible?\\n\\n> Might\\n>even be a vector (\"strokewriter\") display, in which case the lower limit\\n>on image time is anyone\\'s guess (and is probably phosphor-persistence limited).\\n\\nBack in high school I worked as a lab assistant for a bunch of experimental\\npsychologists at Bell Labs.  When they were doing visual perception and\\nmemory experiments, they used vector-type displays, with 1-millisecond\\nrefresh rates common.\\n\\nSo your case of 1/200th sec is quite practical, and the experimenters were\\nprobably sure that it was 5 milliseconds, not 4 or 6 either.\\n\\n>Bob Myers  KC0EW >myers@fc.hp.com \\n\\nSteve\\n-- \\nSteve Tell       tell@cs.unc.edu H: 919 968 1792  | #5L Estes Park apts\\nUNC Chapel Hill Computer Science W: 919 962 1845  | Carrboro NC 27510\\nEngineering is a _lot_ like art:  Some circuits are like lyric poems, some\\nare like army manuals, and some are like The Hitchhiker\\'s Guide to the Galaxy..\\n',\n",
       " 'From: lpa8921@tamuts.tamu.edu (Louis Paul Adams)\\nSubject: Re: Number for Applied Engineering\\nOrganization: Texas A&M University, College Station\\nLines: 9\\nNNTP-Posting-Host: tamuts.tamu.edu\\n\\n>Anyone have a phone number for Applied Engineering so I can give them\\n>a call?\\n\\n\\nAE is in Dallas...try 214/241-6060 or 214/241-0055.  Tech support may be on\\ntheir own line, but one of these should get you started.\\n\\nGood luck!\\n\\n',\n",
       " \"From: dchhabra@stpl.ists.ca (Deepak Chhabra)\\nSubject: Re: Atlanta Hockey Hell!!\\nNntp-Posting-Host: stpl.ists.ca\\nOrganization: Solar Terresterial Physics Laboratory, ISTS\\nLines: 24\\n\\nIn article <0foVj7i00WB4MIUmht@andrew.cmu.edu> Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu> writes:\\n>\\n>Well, it's not that bad. But I am still pretty pissed of at the\\n>local ABC coverage. They cut off the first half hour of coverage by playing\\n\\n[stuff deleted]\\n\\nOk, here's the solution to your problem.  Move to Canada.  Yesterday I was able\\nto watch FOUR games...the NJ-PITT at 1:00 on ABC, LA-CAL at 3:00 (CBC), \\nBUFF-BOS at 7:00 (TSN and FOX), and MON-QUE at 7:30 (CBC).  I think that if\\neach series goes its max I could be watching hockey playoffs for 40-some odd\\nconsecutive nights (I haven't counted so that's a pure guess).\\n\\nI have two tv's in my house, and I set them up side-by-side to watch MON-QUE\\nand keep an eye on BOS-BUFF at the same time.  I did the same for the two\\nafternoon games.\\n\\nBtw, those ABC commentaters were great!  I was quite impressed; they seemed\\nto know that their audience wasn't likely to be well-schooled in hockey lore\\nand they did an excellent job.  They were quite impartial also, IMO.\\n\\n\\n\\ndchhabra@stpl.ists.ca (not suffering from a shortage of hockey here)\\n\",\n",
       " \"From: dchhabra@stpl.ists.ca (Deepak Chhabra)\\nSubject: Re: Goalie masks\\nNntp-Posting-Host: stpl.ists.ca\\nOrganization: Solar Terresterial Physics Laboratory, ISTS\\nLines: 15\\n\\nIn article <C5sqz3.EG8@acsu.buffalo.edu> hammerl@acsu.buffalo.edu (Valerie S. Hammerl) writes:\\n\\n>>[...] and I'll give Fuhr's new one an honourable mention, although I haven't\\n>>seen it closely yet (it looked good from a distance!).  \\n\\n>This is the new Buffalo one, the second since he's been with the\\n>Sabres?  I recall a price tag of over $700 just for the paint job on\\n>that mask, and a total price of almost $1500.  Ouch.  \\n\\nYeah, it's the second one.  And I believe that price too.  I've been trying\\nto get a good look at it on the Bruin-Sabre telecasts, and wow! does it ever\\nlook good.  Whoever did that paint job knew what they were doing.  And given\\nFuhr's play since he got it, I bet the Bruins are wishing he didn't have it:)\\n\\n--\\n\",\n",
       " 'From: arromdee@jyusenkyou.cs.jhu.edu (Ken Arromdee)\\nSubject: Re: Christians above the Law? was Clarification of pe\\nOrganization: Johns Hopkins University CS Dept.\\nLines: 13\\n\\nIn article <C61Kow.E4z@mailer.cc.fsu.edu> dlecoint@garnet.acns.fsu.edu (Darius_Lecointe) writes:\\n>>Jesus was a JEW, not a Christian.\\n\\nIf a Christian means someone who believes in the divinity of Jesus, it is safe\\nto say that Jesus was a Christian.\\n--\\n\"On the first day after Christmas my truelove served to me...  Leftover Turkey!\\nOn the second day after Christmas my truelove served to me...  Turkey Casserole\\n    that she made from Leftover Turkey.\\n[days 3-4 deleted] ...  Flaming Turkey Wings! ...\\n   -- Pizza Hut commercial (and M*tlu/A*gic bait)\\n\\nKen Arromdee (arromdee@jyusenkyou.cs.jhu.edu)\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is how the features look like\n",
    "list(newsgroups.data)[1:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "kz27FYD_y5cK",
    "outputId": "03bfa5a5-0ffa-4575-f4bc-69abeb4598bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is how the target looks like\n",
    "newsgroups.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F91JKozKy5cN"
   },
   "source": [
    "![alt text](https://)**2.1.1) Most common data-proprocessing steps**\n",
    "![alt text](http://cs.mcgill.ca/~sthaku3/other_media/COMP551_tutorial_2/Preprocessing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0P2Htdpmy5cO"
   },
   "source": [
    "**2.1.1.1) Train-test split **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vX07SHuvy5cP",
    "outputId": "11d80cfb-95e3-4489-9372-208242754774"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15076\n"
     ]
    }
   ],
   "source": [
    "## Simple way to do split would be to use scikit-learn's `train_test_split` method\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, train_size=0.8, test_size=0.2)\n",
    "print(len(X_train))\n",
    "# train tesprint(len(y_train))\n",
    "# print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1020
    },
    "colab_type": "code",
    "id": "ppUUSZ-My5cS",
    "outputId": "e0754bfc-8ef3-4bdc-8ce2-f6fe836179c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: jmd@cube.handheld.com (Jim De Arras)\n",
      "Subject: Re: CLINTON JOINS LIST OF GENOCIDAL SOCIALIST LEADERS\n",
      "Organization: Hand Held Products, Inc.\n",
      "Lines: 51\n",
      "Distribution: world\n",
      "NNTP-Posting-Host: dale.handheld.com\n",
      "\n",
      "In article <1993Apr23.153005.8237@starbase.trincoll.edu>  () writes:\n",
      "> In article <1r6h4vINN844@clem.handheld.com>, jmd@cube.handheld.com (Jim De\n",
      "> Arras) wrote:\n",
      "> >   \n",
      "> > You seem to make two points.  No one ultimately oversees the federal  \n",
      "agencies  \n",
      "> > you mention, and since Koresh \"apparently\" has a different view point from  \n",
      "your  \n",
      "> > Baptist upbringing, then he is not worthy of protection from religious  \n",
      "> > persecution.  As to being the Messiah, is not Christ within us all?\n",
      "> > \n",
      "> > Must be comforting to belong to a government approved religion.\n",
      "> > \n",
      "> > Baptists are a cult, two, BTW, under most of the definitions in the  \n",
      "dictionary  \n",
      "> > of \"cult\".\n",
      "> > \n",
      "> \n",
      "> I've yet to meet a group of Baptists who were stockpiling Cambell's soup\n",
      "> and M-16's/AR-15's and banging/marrying thirteen yuear olds. \n",
      "\n",
      "I don't recall saying Baptists do any of that.  Though I suppose some do.  And  \n",
      "none of them are listed in the dictionary as characteristics of a cult.  My  \n",
      "mother stockpiled Campbells soup when it was on sale.  \n",
      "\n",
      "> You're a sorry\n",
      "> son of a bitch if you can't draw a distinction between these two things.\n",
      "\n",
      "You are an intolerent, foul-mouthed human.  You sound like you are ready to  \n",
      "join the KKK or neo-nazis, with a narrow mind like yours.\n",
      "\n",
      "> People like you cheapen our constitution by using it to defend sociopaths\n",
      "> who aren't deserved of it. Get a life and chill on the paranoia.\n",
      "> \n",
      "\n",
      "Far from it, I defend the rights of anyone to be different under our  \n",
      "constitution, which was formed in part to protect religious cults which had  \n",
      "been persecuted in England before migrating here to be free.  You are the one  \n",
      "endangering our constitution.\n",
      "\n",
      "> joe.kusmierczak@mail.trincoll.edu\n",
      "\n",
      "Jim\n",
      "--\n",
      "jmd@handheld.com\n",
      "-------------------------------------------------------------------------------\n",
      "\"I'm always rethinking that.  There's never been a day when I haven't rethought  \n",
      "that.  But I can't do that by myself.\"  Bill Clinton  6 April 93\n",
      "\"If I were an American, as I am an Englishman, while a foreign troop was landed  \n",
      "in my country, I never would lay down my arms,-never--never--never!\"\n",
      "WILLIAM PITT, EARL OF CHATHAM 1708-1778 18 Nov. 1777\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24q_UHoUy5ca"
   },
   "source": [
    "**2.1.1.2) Vectorization of textual features (applicable only for textual features)**\n",
    "\n",
    "A very simple approach to represent textual features such as documents or sentences as numerical value is to use each word as an atomic type and as a basis for a vector space. For example imagine a world where there exist only 3 words: “Apple”, “Orange”, and “Banana” and every sentence or document is made of them. They become the basis of a 3 dimensional vector space:\n",
    "\n",
    "```\n",
    "Apple  ==>> [1,0,0]\n",
    "Banana ==>> [0,1,0]\n",
    "Orange ==>> [0,0,1]\n",
    "```\n",
    "\n",
    "This representation is called **one_hot** as it is always a vector of zeros with 1 on the position of the word.\n",
    "\n",
    "Then a “sentence” or a “document” is simply the linear combination of these vectors where the number of the counts of appearance of the words is the coefficient along that dimension. For example:\n",
    "\n",
    "```\n",
    "d3 = \"Apple Orange Orange Apple\" ==>> [2,0,2]\n",
    "d4 = \"Apple Banana Apple Banana\" ==>> [2,2,0]\n",
    "d1 = \"Banana Apple Banana Banana Banana Apple\" ==>> [2,4,0]\n",
    "d2 = \"Banana Orange Banana Banana Orange Banana\" ==>> [0,4,2]\n",
    "d5 = \"Banana Apple Banana Banana Orange Banana\" ==>> [1,4,1]\n",
    "```\n",
    "\n",
    "Since, our toy dataset also has textual features, we'll have to vectorize them. But let's do train-test split first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B_K5_J0XYr2-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eKljh0LHy5cb"
   },
   "source": [
    "## Now we transform text into vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "0opTm5sxy5cd",
    "outputId": "87c15ed2-a0df-4bc5-9973-5ea6f04c608f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "print(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2p4yikZy5cg"
   },
   "outputs": [],
   "source": [
    "vectors_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# and we do the same for test data. remember to use the same vectorizer, only transform (why?)\n",
    "vectors_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "meXvyDGZy5ck",
    "outputId": "30ee4265-9025-459c-ece6-cd7ac390f85d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15076, 143453)\n"
     ]
    }
   ],
   "source": [
    "print(vectors_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "7Ad5ceg2y5co",
    "outputId": "8a9a8e40-6be3-45c3-f4cf-17a9f971e28f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "print(CountVectorizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aMfEwdWyy5cr"
   },
   "source": [
    "**2.1.1.3) Tf–idf term weighting(only for textual features)**\n",
    "\n",
    "In a large text corpus, some words are quite common (e.g. “the”, “a”, “is” in English). These words don't always convery meaningful information. However, if we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\n",
    "\n",
    "In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform.''.\n",
    "\n",
    "tf means term-frequency while tf–idf means term-frequency times inverse document-frequency:\n",
    "\n",
    "$$\\text{tf-idf} = \\text{tf}(t,d)\\  \\text{X} \\ \\text{idf}(t)$$ \n",
    "  where :\n",
    "- $t$ =  term/token/word \n",
    " \n",
    "- $d$ = document or a paragraph\n",
    "\n",
    "- The $\\text{tf}(n,d)$ is the number of times a token $t$ appears in a document $d$\n",
    "\n",
    "- The idf is calculted as using the following formula:\n",
    "  $$log\\frac{n_{d}}{1+ \\text{df}(d,t)}$$\n",
    "  - where\n",
    "   - $n_{d}$ is the total number of documents\n",
    "   - $\\text{df}(d,t)$ is the number of documents $d$ that contain term $t$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "hvEttHmBy5cs",
    "outputId": "4e0048a0-66c5-4991-bcc1-146ad69ec8b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"From: pdc@dcs.ed.ac.uk (Paul Crowley)\\nSubject: Re: Secret algorithm [Re: Clipper Chip and crypto key-escrow]\\nReply-To: pdc@dcs.ed.ac.uk (Paul Crowley)\\nOrganization: Edinburgh University\\nLines: 11\\n\\nQuoting pla@sktb.demon.co.uk in article <8AOHOnj024n@sktb.demon.co.uk>:\\n>You have every reason to be scared shitless.  Take a look at the records\\n>of McCarthy, Hoover (J. Edgar, not the cleaner - though they both excelled at\\n>sucking) and Nixon.\\n\\nHistory does not record whether J. Edgar Hoover was any good at sucking.\\nAs for the cleaners, I'll stick with my 850W Electrolux and damn the\\ncarpet.\\n  __                                  _____\\n\\\\/ o\\\\ Paul Crowley   pdc@dcs.ed.ac.uk \\\\\\\\ //\\n/\\\\__/ Trust me. I know what I'm doing. \\\\X/  Fold a fish for Jesus!\\n\"]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "vectors_train_idf = tf_idf_vectorizer.fit_transform(X_train)\n",
    "vectors_test_idf = vectorizer.transform(X_test)\n",
    "print(X_train[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890
    },
    "colab_type": "code",
    "id": "UhbIA0EWy5cx",
    "outputId": "f1ddea38-6436-4381-8099-efb6aa7a6d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15076, 143453)\n",
      "  (0, 63454)\t0.0288135123406339\n",
      "  (0, 42332)\t0.3419966030117869\n",
      "  (0, 82767)\t0.17637085176314105\n",
      "  (0, 45539)\t0.16955966078919996\n",
      "  (0, 56164)\t0.04095097076510266\n",
      "  (0, 119892)\t0.20762215355290042\n",
      "  (0, 123298)\t0.01440675617031695\n",
      "  (0, 111507)\t0.10915139419951263\n",
      "  (0, 51392)\t0.16343501905746047\n",
      "  (0, 109640)\t0.30924922353751205\n",
      "  (0, 128135)\t0.17165611334171327\n",
      "  (0, 53326)\t0.03568723079562396\n",
      "  (0, 132658)\t0.04340824223962878\n",
      "  (0, 97010)\t0.02676727411286295\n",
      "  (0, 105355)\t0.025980159325386727\n",
      "  (0, 71356)\t0.05322077009932647\n",
      "  (0, 37181)\t0.11606048663269346\n",
      "  (0, 99961)\t0.01499080819659299\n",
      "  (0, 45512)\t0.1354047003592674\n",
      "  (0, 122153)\t0.090684438174631\n",
      "  (0, 131875)\t0.027879548988000137\n",
      "  (0, 62876)\t0.09287370705699145\n",
      "  (0, 45460)\t0.08970332914063169\n",
      "  (0, 45130)\t0.05959052792161008\n",
      "  (0, 22153)\t0.11399886767059564\n",
      "  :\t:\n",
      "  (15075, 52850)\t0.061604288397687036\n",
      "  (15075, 95673)\t0.40231158365764463\n",
      "  (15075, 131464)\t0.11063485206848919\n",
      "  (15075, 109607)\t0.1013221165173675\n",
      "  (15075, 104055)\t0.1013221165173675\n",
      "  (15075, 36688)\t0.09287894653581544\n",
      "  (15075, 19166)\t0.11480886600103472\n",
      "  (15075, 71465)\t0.17997360275654964\n",
      "  (15075, 121341)\t0.11387246880353727\n",
      "  (15075, 53896)\t0.11063485206848919\n",
      "  (15075, 6930)\t0.11299285884397152\n",
      "  (15075, 31564)\t0.10739723533344112\n",
      "  (15075, 80150)\t0.11387246880353727\n",
      "  (15075, 85181)\t0.19139195919509364\n",
      "  (15075, 42440)\t0.11930894131318291\n",
      "  (15075, 45048)\t0.22432708102910057\n",
      "  (15075, 46166)\t0.3444265980031042\n",
      "  (15075, 85177)\t0.10381551264945924\n",
      "  (15075, 139957)\t0.11930894131318291\n",
      "  (15075, 16)\t0.12222049666862833\n",
      "  (15075, 13851)\t0.12222049666862833\n",
      "  (15075, 128028)\t0.12392942495761532\n",
      "  (15075, 34521)\t0.13816039504423885\n",
      "  (15075, 100698)\t0.12586684805978093\n",
      "  (15075, 131469)\t0.13816039504423885\n"
     ]
    }
   ],
   "source": [
    "print(vectors_train_idf.shape)\n",
    "print(vectors_train_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FMp472oFy5c1"
   },
   "source": [
    "**2.1.1.4) Normalization**::\n",
    "While not mandatory, normalization usually improves the performance of the learner significantly. It ensures that the learner learns from the data on similar scales across features. There are many ways of normalizing the data.\n",
    "This will also be covered in detail in lecture *Feature Construction and Selection*.\n",
    "\n",
    "For now we'll stick to the default *l2* provided by scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W3-7AWYey5c3"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "normalizer_train = Normalizer().fit(X=vectors_train)\n",
    "vectors_train_normalized = normalizer_train.transform(vectors_train)\n",
    "vectors_test_normalized = normalizer_train.transform(vectors_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CRTjpeXTy5c6"
   },
   "source": [
    "**2.1.1.5) One-hot encoding of labels (for classification problems only)**\n",
    "The integer nature of the labels is not amenable for classification tasks. However, scikit-learn above internally handles the integer nature of our labels. In most other cases, the programmers need to represent them in a format that allows handling them explicitly. One such popular format is one-hot encoding. This one-hot encoding works similar to as explained in section 2.1.1.2. So, we are only refering to a function [here](http://scikit-learn.org/stable/modules/preprocessing_targets.html#) that does that for you but for labels in the context of classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zxfTNkXoy5c6"
   },
   "source": [
    "![alt text](http://cs.mcgill.ca/~sthaku3/other_media/COMP551_tutorial_2/training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jxif9C3my5c7"
   },
   "outputs": [],
   "source": [
    "## Now we instantiate the classifier. Remember this can be any classifier, even the one you make.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "osRWgUIMy5dI"
   },
   "source": [
    "**2.1.2.1) Fit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "jvqyFeOpy5dL",
    "outputId": "a668a66a-ebc9-4e68-aa77-b6fc8fcb42d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Scikit Learn API is very simple and straightforward, which contains the basic commands:\n",
    "## `fit` to learn your parameters\n",
    "clf.fit(vectors_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vxsozhnqy5dR"
   },
   "source": [
    "**2.1.2.2) Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890
    },
    "colab_type": "code",
    "id": "j9N0dW68y5dR",
    "outputId": "c4a2bc76-7456-4ff0-be45-ab29a37289e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3778)\t1\n",
      "  (0, 7420)\t1\n",
      "  (0, 15092)\t1\n",
      "  (0, 15282)\t1\n",
      "  (0, 16870)\t1\n",
      "  (0, 19961)\t1\n",
      "  (0, 21129)\t1\n",
      "  (0, 24295)\t1\n",
      "  (0, 27731)\t1\n",
      "  (0, 27765)\t1\n",
      "  (0, 27777)\t1\n",
      "  (0, 27894)\t6\n",
      "  (0, 27895)\t1\n",
      "  (0, 27994)\t1\n",
      "  (0, 28380)\t1\n",
      "  (0, 28495)\t1\n",
      "  (0, 30049)\t5\n",
      "  (0, 30254)\t1\n",
      "  (0, 30268)\t1\n",
      "  (0, 30362)\t1\n",
      "  (0, 30479)\t1\n",
      "  (0, 30599)\t1\n",
      "  (0, 30690)\t2\n",
      "  (0, 30828)\t13\n",
      "  (0, 31378)\t1\n",
      "  :\t:\n",
      "  (3769, 115901)\t1\n",
      "  (3769, 120167)\t1\n",
      "  (3769, 120302)\t1\n",
      "  (3769, 120444)\t1\n",
      "  (3769, 120952)\t1\n",
      "  (3769, 121125)\t3\n",
      "  (3769, 122557)\t2\n",
      "  (3769, 123298)\t1\n",
      "  (3769, 124122)\t1\n",
      "  (3769, 126743)\t1\n",
      "  (3769, 126770)\t3\n",
      "  (3769, 126785)\t14\n",
      "  (3769, 126984)\t1\n",
      "  (3769, 127965)\t1\n",
      "  (3769, 131847)\t3\n",
      "  (3769, 132694)\t1\n",
      "  (3769, 134289)\t1\n",
      "  (3769, 136942)\t1\n",
      "  (3769, 137227)\t1\n",
      "  (3769, 137570)\t1\n",
      "  (3769, 137880)\t4\n",
      "  (3769, 138409)\t3\n",
      "  (3769, 138580)\t1\n",
      "  (3769, 139253)\t1\n",
      "  (3769, 141734)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([18, 12,  0, ..., 13, 11,  4])"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We get the predicted class\n",
    "print(vectors_test)\n",
    "y_pred = clf.predict(vectors_test)\n",
    "## So now we see we have a set of predictions. \n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ntuSHtL3y5dU"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "To see how good or bad your classifier did, you should check the predictions with the gold standard dataset. The cool thing about scikit-learn is that it gives you several metrics to do so. You can even use your own classifier and use the list of predicted classes and gold standard classes to compare.\n",
    "\n",
    "![alt text](http://cs.mcgill.ca/~sthaku3/other_media/COMP551_tutorial_2/metrics.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HYmHZ-rTy5dV"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-zhRLJ4y5dX"
   },
   "source": [
    "The `metrics` class provides a set of useful metrics you can use for your needs. For any classification task, you need to report mainly these metrics:\n",
    "\n",
    "- Accuracy : (TP + TN) / (TP + TN + FP + FN)\n",
    "- Precision : TP / (TP + FP)\n",
    "- F1 Score : 2TP / (2TP + FP + FN)\n",
    "- Recall Score : TP / (TP + FN)\n",
    "\n",
    "Remember, when we do multi-class classification, we use it in `macro` average mode, where we calculate metrics for each label, and find their unweighted mean. You can also instead use other averaging modes such as `micro`, `weighted`, `samples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3-_XmH8cy5dZ",
    "outputId": "325cda3d-82ca-4654-d032-d40aa90b63fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.903448275862069"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yWsflETwy5df",
    "outputId": "54091dfc-eac6-431d-e1ce-203b8572ce4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9046498723495265"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BYujbhphNWWE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YMCkGh6_y5dj",
    "outputId": "ef0d12f0-d991-48fc-d404-5f5edc9ff6e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.902861326959733"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UKf3L8Wfy5dm",
    "outputId": "471eaa50-d149-4a57-b69b-23fd89c51a33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.902009043513538"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "id": "2fYrfP3Wy5dq",
    "outputId": "3ffaa8fd-8745-4b4f-886b-2eb491abff5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.90      0.91       168\n",
      "          1       0.82      0.84      0.83       196\n",
      "          2       0.87      0.84      0.85       195\n",
      "          3       0.82      0.81      0.81       200\n",
      "          4       0.89      0.89      0.89       213\n",
      "          5       0.90      0.91      0.91       199\n",
      "          6       0.82      0.90      0.86       186\n",
      "          7       0.89      0.88      0.89       187\n",
      "          8       0.97      0.94      0.95       200\n",
      "          9       0.93      0.94      0.93       199\n",
      "         10       0.93      0.97      0.95       191\n",
      "         11       0.98      0.95      0.97       198\n",
      "         12       0.84      0.83      0.84       191\n",
      "         13       0.92      0.93      0.93       224\n",
      "         14       0.95      0.93      0.94       201\n",
      "         15       0.91      0.97      0.94       182\n",
      "         16       0.90      0.95      0.93       180\n",
      "         17       0.98      0.97      0.97       178\n",
      "         18       0.93      0.88      0.90       157\n",
      "         19       0.92      0.81      0.86       125\n",
      "\n",
      "avg / total       0.90      0.90      0.90      3770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## You can show all of this in a single call\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "A0lwFEchy5dw",
    "outputId": "e77d9c4b-d857-45f8-f877-ae01e370eee9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88653655 0.89002981 0.88594164 0.87607973 0.89195479]\n"
     ]
    }
   ],
   "source": [
    "### Cross Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "scores = cross_val_score(clf, vectors_train, y_train, cv=5)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bZjbQhbFy5dz"
   },
   "source": [
    "## Grid Search\n",
    "\n",
    "When you are first searching for the best hyperparamters, its a good first strategy to run a grid search with the choice of hyperparameters to see which ones work the best for your dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "yGY_4QEOy5d0",
    "outputId": "442ec7d1-1daf-43ef-8c83-99b7180a6673"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [0.88611037 0.84770496 0.86700716 0.87609445 0.87894667]\n",
      "scores_std [0.00547919 0.00674491 0.00838737 0.00750473 0.00641575]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "tuned_parameters = [{'alpha': [0.01, 1, 0.5, 0.2, 0.1]}]\n",
    "\n",
    "n_folds = 5\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=n_folds, refit=False)\n",
    "clf.fit(vectors_train, y_train)\n",
    "scores = clf.cv_results_['mean_test_score']\n",
    "scores_std = clf.cv_results_['std_test_score']\n",
    "print('scores:',scores)\n",
    "print('scores_std',scores_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U5GvObiLy5d9"
   },
   "source": [
    "### Choice of Classifier\n",
    "![Choosing the right estimator](http://scikit-learn.org/stable/_static/ml_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "goA4J7bEy5d9"
   },
   "source": [
    "### References\n",
    "\n",
    "1.[Scikit Learn](http://scikit-learn.org/)\n",
    "\n",
    "2.[Comp-551-tutorial(Winter-2018)](https://colab.research.google.com/drive/1SDc_x307LwqNucOclQinOdlDvawZdeo9#scrollTo=Cpjv6BG1ctcK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFXcwe3iy5d-"
   },
   "source": [
    "### Useful Links\n",
    "\n",
    "1. ROC Curve: http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "2. https://people.duke.edu/~ccc14/sta-663/BlackBoxOptimization.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tutorial_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
